{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#  Import and read the diabetes_data.csv.\n",
    "import pandas as pd\n",
    "diabetes_data_df = pd.read_csv('https://raw.githubusercontent.com/ComfyKoala/diabetes-classification/main/Stephen/diabetes_data.csv')\n",
    "# Display the first few rows\n",
    "diabetes_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the columns in the data set\n",
    "print(diabetes_data_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Non-Beneficial Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, 'PatientID' and 'DoctorInCharge'.\n",
    "diabetes_data_clean_df = diabetes_data_df.drop(columns=['PatientID', 'DoctorInCharge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes_data_clean_df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning w/ Logarithmic Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns: 'Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity', 'DietQuality', 'SleepQuality', 'SystolicBP', 'DiastolicBP', 'FastingBloodSugar', 'HbA1c', 'SerumCreatinine', 'BUNLevels', 'CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL', 'CholesterolTriglycerides', 'FatigueLevels', 'QualityOfLifeScore', 'MedicalCheckupsFrequency', 'MedicationAdherence', 'HealthLiteracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_and_bin(diabetes_data_clean_df, columns, num_bins=10, drop_original=True):\n",
    "    \"\"\"\n",
    "    Logarithmically transform specified columns, bin them, and optionally drop the original columns.\n",
    "\n",
    "    Parameters:\n",
    "        diabetes_data_clean_df (pd.DataFrame): The DataFrame to be transformed.\n",
    "        columns (list): List of columns to log-transform and bin.\n",
    "        num_bins (int): The number of bins to create. Default is 10.\n",
    "        drop_original (bool): If True, drop the original columns. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed DataFrame with binned columns.\n",
    "    \"\"\"\n",
    "\n",
    "    transformed_df = diabetes_data_clean_df.copy()\n",
    "\n",
    "    for col in columns:\n",
    "        # Log-transform the specified column\n",
    "        transformed_df[f'Log_{col}'] = np.log1p(transformed_df[col])\n",
    "\n",
    "        # Create bins based on the log-transformed values\n",
    "        log_bins = pd.cut(transformed_df[f'Log_{col}'], bins=num_bins)\n",
    "\n",
    "        # Access the bin edges from the categorical object\n",
    "        bin_edges_log_scale = log_bins.cat.categories\n",
    "\n",
    "        # Reverse the logarithmic transformation to get the bin edges on the original scale\n",
    "        bin_edges = np.expm1([bin_edges_log_scale.left.min()] + list(bin_edges_log_scale.right))\n",
    "\n",
    "        # Ensure bin edges are unique\n",
    "        bin_edges = np.unique(bin_edges)\n",
    "\n",
    "        # Create bin labels corresponding to the reversed log-transformed bin edges\n",
    "        bin_labels = [f'{int(bin_edges[i])}-{int(bin_edges[i+1]-1)}' for i in range(len(bin_edges)-1)]\n",
    "\n",
    "        # Assign the custom labels and create the binned column in the original DataFrame\n",
    "        transformed_df[f'{col}_Bins'] = pd.cut(transformed_df[col], bins=bin_edges, labels=bin_labels, right=False, ordered=False)\n",
    "\n",
    "        # Optionally drop the original and temporary Log column\n",
    "        if drop_original:\n",
    "            transformed_df.drop(columns=[col, f'Log_{col}'], inplace=True)\n",
    "        else:\n",
    "            transformed_df.drop(columns=[f'Log_{col}'], inplace=True)\n",
    "\n",
    "    return transformed_df\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'diabetes_data_clean_df' is already defined and has the specified columns\n",
    "columns_to_transform = ['Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity', 'DietQuality',\n",
    "                        'SleepQuality', 'SystolicBP', 'DiastolicBP', 'FastingBloodSugar', 'HbA1c',\n",
    "                        'SerumCreatinine', 'BUNLevels', 'CholesterolTotal', 'CholesterolLDL',\n",
    "                        'CholesterolHDL', 'CholesterolTriglycerides', 'FatigueLevels',\n",
    "                        'QualityOfLifeScore', 'MedicalCheckupsFrequency', 'MedicationAdherence',\n",
    "                        'HealthLiteracy']\n",
    "\n",
    "transformed_df = log_transform_and_bin(diabetes_data_clean_df, columns_to_transform)\n",
    "\n",
    "# Display the first few rows of the transformed DataFrame\n",
    "print(transformed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "transformed_dummies_df = pd.get_dummies(transformed_df, columns=['Ethnicity', 'SocioeconomicStatus', 'EducationLevel',\n",
    "       'Age_Bins', 'BMI_Bins', 'AlcoholConsumption_Bins',\n",
    "       'PhysicalActivity_Bins', 'DietQuality_Bins', 'SleepQuality_Bins',\n",
    "       'SystolicBP_Bins', 'DiastolicBP_Bins', 'FastingBloodSugar_Bins',\n",
    "       'HbA1c_Bins', 'SerumCreatinine_Bins', 'BUNLevels_Bins',\n",
    "       'CholesterolTotal_Bins', 'CholesterolLDL_Bins', 'CholesterolHDL_Bins',\n",
    "       'CholesterolTriglycerides_Bins', 'FatigueLevels_Bins',\n",
    "       'QualityOfLifeScore_Bins', 'MedicalCheckupsFrequency_Bins',\n",
    "       'MedicationAdherence_Bins', 'HealthLiteracy_Bins'])\n",
    "transformed_dummies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Target (y) and Features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = transformed_dummies_df[\"Diagnosis\"]\n",
    "X = transformed_dummies_df.drop(columns=\"Diagnosis\")\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=10,\n",
    "                                                    stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=6, activation=\"relu\", input_dim=n_features))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=3, activation=\"sigmoid\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights every five epochs.\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_weights_epoch_{epoch:02d}.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_freq=5 * len(X_train_scaled)\n",
    ")\n",
    "\n",
    "# Train the model and pass the callback\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history)\n",
    "\n",
    "# Increase the index by 1 to match the number of epochs\n",
    "history_df.index += 1\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.plot(y=\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weakest and Strongest Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikeras scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade scipy scikit-learn scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "# Ensure that nn is defined as your Keras model\n",
    "\n",
    "# Define the wrapped model (assuming nn is your Keras Sequential model)\n",
    "wrapped_nn = KerasClassifier(model=nn, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Train the wrapped model\n",
    "wrapped_nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compute permutation importance\n",
    "result = permutation_importance(wrapped_nn, X_test_scaled, y_test, n_repeats=10, random_state=10)\n",
    "\n",
    "# Get feature importances\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Ensure X_test_scaled is a DataFrame or convert it\n",
    "if not isinstance(X_test_scaled, pd.DataFrame):\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Sort the features by importance in ascending order (weakest to strongest)\n",
    "sorted_indices = np.argsort(importances)\n",
    "\n",
    "# Get the top 20 weakest features\n",
    "top_20_weakest_indices = sorted_indices[:20]\n",
    "top_20_weakest_features = X_test_scaled.columns[top_20_weakest_indices]\n",
    "top_20_weakest_importances = importances[top_20_weakest_indices]\n",
    "\n",
    "# Display the weakest features and their importances\n",
    "for feature, importance in zip(top_20_weakest_features, top_20_weakest_importances):\n",
    "    print(f\"Weakest feature: {feature} with importance {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the wrapped model (assuming nn is your Keras Sequential model)\n",
    "wrapped_nn = KerasClassifier(model=nn, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Train the wrapped model\n",
    "wrapped_nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compute permutation importance\n",
    "result = permutation_importance(wrapped_nn, X_test_scaled, y_test, n_repeats=10, random_state=10)\n",
    "\n",
    "# Get feature importances\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Ensure X_test_scaled is a DataFrame or convert it\n",
    "if not isinstance(X_test_scaled, pd.DataFrame):\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Sort the features by importance in descending order (strongest to weakest)\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Get the top 20 strongest features\n",
    "top_20_strongest_indices = sorted_indices[:20]\n",
    "top_20_strongest_features = X_test_scaled.columns[top_20_strongest_indices]\n",
    "top_20_strongest_importances = importances[top_20_strongest_indices]\n",
    "\n",
    "# Display the strongest features and their importances\n",
    "for feature, importance in zip(top_20_strongest_features, top_20_strongest_importances):\n",
    "    print(f\"Strongest feature: {feature} with importance {importance}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
